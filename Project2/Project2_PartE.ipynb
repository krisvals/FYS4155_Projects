{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e351149c-39fd-4571-a52b-57164fcbab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Kristen/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import autograd.numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "from autograd import grad, elementwise_grad\n",
    "from random import random, seed\n",
    "from copy import deepcopy, copy\n",
    "from typing import Tuple, Callable\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "299359eb-1a0a-4a06-8d24-dec2c4c9b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Franke Function\n",
    "def FrankeFunction(x):\n",
    "    term1 = 0.75 * np.exp(-(0.25 * (9 * x - 2) ** 2))\n",
    "    term2 = 0.75 * np.exp(-((9 * x + 1) ** 2) / 49.0)\n",
    "    term3 = 0.5 * np.exp(-(9 * x - 7) ** 2 / 4.0)\n",
    "    term4 = -0.2 * np.exp(-(9 * x - 4) ** 2)\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "#Generating design matrix\n",
    "def createX(x, n):\n",
    "    N = len(x)\n",
    "    l = n + 1  # Number of elements in beta\n",
    "    X = np.ones((N, l))\n",
    "    for i in range(1, n + 1):\n",
    "        X[:, i] = x**i\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeccdca1-c72d-48d5-b366-4298f17be547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostOLS(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return (1.0 / target.shape[0]) * np.sum((target - X) ** 2)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostLogReg(target):\n",
    "\n",
    "    def func(X):\n",
    "        \n",
    "        return -(1.0 / target.shape[0]) * np.sum(\n",
    "            (target * np.log(X + 10e-10)) + ((1 - target) * np.log(1 - X + 10e-10))\n",
    "        )\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostCrossEntropy(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return -(1.0 / target.size) * np.sum(target * np.log(X + 10e-10))\n",
    "\n",
    "    return func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527d313e-55f5-4031-987e-84066e20c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"error\")\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "def identity(X):\n",
    "    return X\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    try:\n",
    "        return 1.0 / (1 + np.exp(-X))\n",
    "    except FloatingPointError:\n",
    "        return np.where(X > np.zeros(X.shape), np.ones(X.shape), np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    X = X - np.max(X, axis=-1, keepdims=True)\n",
    "    delta = 10e-10\n",
    "    return np.exp(X) / (np.sum(np.exp(X), axis=-1, keepdims=True) + delta)\n",
    "\n",
    "\n",
    "def RELU(X):\n",
    "    return np.where(X > np.zeros(X.shape), X, np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def LRELU(X):\n",
    "    delta = 10e-4\n",
    "    return np.where(X > np.zeros(X.shape), X, delta * X)\n",
    "\n",
    "\n",
    "def derivate(func):\n",
    "    if func.__name__ == \"RELU\":\n",
    "\n",
    "        def func(X):\n",
    "            return np.where(X > 0, 1, 0)\n",
    "\n",
    "        return func\n",
    "\n",
    "    elif func.__name__ == \"LRELU\":\n",
    "\n",
    "        def func(X):\n",
    "            delta = 10e-4\n",
    "            return np.where(X > 0, 1, delta)\n",
    "\n",
    "        return func\n",
    "\n",
    "    else:\n",
    "        return elementwise_grad(func)\n",
    "\n",
    "import autograd.numpy as np\n",
    "\n",
    "class Scheduler:\n",
    "    \"\"\"\n",
    "    Abstract class for Schedulers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta):\n",
    "        self.eta = eta\n",
    "\n",
    "    # should be overwritten\n",
    "    def update_change(self, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # overwritten if needed\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Constant(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        return self.eta * gradient\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Momentum(Scheduler):\n",
    "    def __init__(self, eta: float, momentum: float):\n",
    "        super().__init__(eta)\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        self.change = self.momentum * self.change + self.eta * gradient\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Adagrad(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = np.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + np.sqrt(np.reshape(np.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        return self.eta * gradient * G_t_inverse\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class AdagradMomentum(Scheduler):\n",
    "    def __init__(self, eta, momentum):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = np.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + np.sqrt(np.reshape(np.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        self.change = self.change * self.momentum + self.eta * gradient * G_t_inverse\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class RMS_prop(Scheduler):\n",
    "    def __init__(self, eta, rho):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.second = 0.0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "        self.second = self.rho * self.second + (1 - self.rho) * gradient * gradient\n",
    "        return self.eta * gradient / (np.sqrt(self.second + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.second = 0.0\n",
    "\n",
    "\n",
    "class Adam(Scheduler):\n",
    "    def __init__(self, eta, rho, rho2):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.rho2 = rho2\n",
    "        self.moment = 0\n",
    "        self.second = 0\n",
    "        self.n_epochs = 1\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        self.moment = self.rho * self.moment + (1 - self.rho) * gradient\n",
    "        self.second = self.rho2 * self.second + (1 - self.rho2) * gradient * gradient\n",
    "\n",
    "        moment_corrected = self.moment / (1 - self.rho**self.n_epochs)\n",
    "        second_corrected = self.second / (1 - self.rho2**self.n_epochs)\n",
    "\n",
    "        return self.eta * moment_corrected / (np.sqrt(second_corrected + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_epochs += 1\n",
    "        self.moment = 0\n",
    "        self.second = 0\n",
    "momentum_scheduler = Momentum(eta=1e-3, momentum=0.9)\n",
    "adam_scheduler = Adam(eta=1e-3, rho=0.9, rho2=0.999)\n",
    "class FFNN:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "        Feed Forward Neural Network with interface enabling flexible design of a\n",
    "        nerual networks architecture and the specification of activation function\n",
    "        in the hidden layers and output layer respectively. This model can be used\n",
    "        for both regression and classification problems, depending on the output function.\n",
    "\n",
    "    Attributes:\n",
    "    ------------\n",
    "        I   dimensions (tuple[int]): A list of positive integers, which specifies the\n",
    "            number of nodes in each of the networks layers. The first integer in the array\n",
    "            defines the number of nodes in the input layer, the second integer defines number\n",
    "            of nodes in the first hidden layer and so on until the last number, which\n",
    "            specifies the number of nodes in the output layer.\n",
    "        II  hidden_func (Callable): The activation function for the hidden layers\n",
    "        III output_func (Callable): The activation function for the output layer\n",
    "        IV  cost_func (Callable): Our cost function\n",
    "        V   seed (int): Sets random seed, makes results reproducible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: tuple[int],\n",
    "        hidden_func: Callable = sigmoid,\n",
    "        output_func: Callable = lambda x: x,\n",
    "        cost_func: Callable = CostOLS,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        self.dimensions = dimensions\n",
    "        self.hidden_func = hidden_func\n",
    "        self.output_func = output_func\n",
    "        self.cost_func = cost_func\n",
    "        self.seed = seed\n",
    "        self.weights = list()\n",
    "        self.schedulers_weight = list()\n",
    "        self.schedulers_bias = list()\n",
    "        self.a_matrices = list()\n",
    "        self.z_matrices = list()\n",
    "        self.classification = None\n",
    "\n",
    "        self._set_classification()\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        t: np.ndarray,\n",
    "        scheduler: Scheduler,\n",
    "        batches: int = 1,\n",
    "        epochs: int = 100,\n",
    "        lam: float = 0,\n",
    "        X_val: np.ndarray = None,\n",
    "        t_val: np.ndarray = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            This function performs the training the neural network by performing the feedforward and backpropagation\n",
    "            algorithm to update the networks weights.\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I    X (np.ndarray) : training data\n",
    "            II   t (np.ndarray) : target data\n",
    "            III  scheduler (Scheduler) : specified scheduler (algorithm for optimization of gradient descent)\n",
    "            IV   scheduler_args (list[int]) : list of all arguments necessary for scheduler\n",
    "\n",
    "        Optional Parameters:\n",
    "        ------------\n",
    "            V    batches (int) : number of batches the datasets are split into, default equal to 1\n",
    "            VI   epochs (int) : number of iterations used to train the network, default equal to 100\n",
    "            VII  lam (float) : regularization hyperparameter lambda\n",
    "            VIII X_val (np.ndarray) : validation set\n",
    "            IX   t_val (np.ndarray) : validation target set\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            I   scores (dict) : A dictionary containing the performance metrics of the model.\n",
    "                The number of the metrics depends on the parameters passed to the fit-function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # setup \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        val_set = False\n",
    "        if X_val is not None and t_val is not None:\n",
    "            val_set = True\n",
    "\n",
    "        # creating arrays for score metrics\n",
    "        train_errors = np.empty(epochs)\n",
    "        train_errors.fill(np.nan)\n",
    "        val_errors = np.empty(epochs)\n",
    "        val_errors.fill(np.nan)\n",
    "\n",
    "        train_accs = np.empty(epochs)\n",
    "        train_accs.fill(np.nan)\n",
    "        val_accs = np.empty(epochs)\n",
    "        val_accs.fill(np.nan)\n",
    "\n",
    "        self.schedulers_weight = list()\n",
    "        self.schedulers_bias = list()\n",
    "\n",
    "        batch_size = X.shape[0] // batches\n",
    "\n",
    "        X, t = resample(X, t)\n",
    "\n",
    "        # this function returns a function valued only at X\n",
    "        cost_function_train = self.cost_func(t)\n",
    "        if val_set:\n",
    "            cost_function_val = self.cost_func(t_val)\n",
    "\n",
    "        # create schedulers for each weight matrix\n",
    "        for i in range(len(self.weights)):\n",
    "            self.schedulers_weight.append(copy(scheduler))\n",
    "            self.schedulers_bias.append(copy(scheduler))\n",
    "\n",
    "        print(f\"{scheduler.__class__.__name__}: Eta={scheduler.eta}, Lambda={lam}\")\n",
    "\n",
    "        try:\n",
    "            for e in range(epochs):\n",
    "                for i in range(batches):\n",
    "                    # allows for minibatch gradient descent\n",
    "                    if i == batches - 1:\n",
    "                        # If the for loop has reached the last batch, take all thats left\n",
    "                        X_batch = X[i * batch_size :, :]\n",
    "                        t_batch = t[i * batch_size :, :]\n",
    "                    else:\n",
    "                        X_batch = X[i * batch_size : (i + 1) * batch_size, :]\n",
    "                        t_batch = t[i * batch_size : (i + 1) * batch_size, :]\n",
    "\n",
    "                    self._feedforward(X_batch)\n",
    "                    \n",
    "\n",
    "                # reset schedulers for each epoch (some schedulers pass in this call)\n",
    "                for scheduler in self.schedulers_weight:\n",
    "                    scheduler.reset()\n",
    "\n",
    "                for scheduler in self.schedulers_bias:\n",
    "                    scheduler.reset()\n",
    "\n",
    "                # computing performance metrics\n",
    "                pred_train = self.predict(X)\n",
    "                train_error = cost_function_train(pred_train)\n",
    "\n",
    "                train_errors[e] = train_error\n",
    "                if val_set:\n",
    "                    \n",
    "                    pred_val = self.predict(X_val)\n",
    "                    val_error = cost_function_val(pred_val)\n",
    "                    val_errors[e] = val_error\n",
    "\n",
    "                if self.classification:\n",
    "                    train_acc = self._accuracy(self.predict(X), t)\n",
    "                    train_accs[e] = train_acc\n",
    "                    if val_set:\n",
    "                        val_acc = self._accuracy(pred_val, t_val)\n",
    "                        val_accs[e] = val_acc\n",
    "\n",
    "                # printing progress bar\n",
    "                progression = e / epochs\n",
    "                print_length = self._progress_bar(\n",
    "                    progression,\n",
    "                    train_error=train_errors[e],\n",
    "                    train_acc=train_accs[e],\n",
    "                    val_error=val_errors[e],\n",
    "                    val_acc=val_accs[e],\n",
    "                )\n",
    "        except KeyboardInterrupt:\n",
    "            # allows for stopping training at any point and seeing the result\n",
    "            pass\n",
    "\n",
    "        # visualization of training progression (similiar to tensorflow progression bar)\n",
    "        sys.stdout.write(\"\\r\" + \" \" * print_length)\n",
    "        sys.stdout.flush()\n",
    "        self._progress_bar(\n",
    "            1,\n",
    "            train_error=train_errors[e],\n",
    "            train_acc=train_accs[e],\n",
    "            val_error=val_errors[e],\n",
    "            val_acc=val_accs[e],\n",
    "        )\n",
    "        sys.stdout.write(\"\")\n",
    "\n",
    "        # return performance metrics for the entire run\n",
    "        scores = dict()\n",
    "\n",
    "        scores[\"train_errors\"] = train_errors\n",
    "\n",
    "        if val_set:\n",
    "            scores[\"val_errors\"] = val_errors\n",
    "\n",
    "        if self.classification:\n",
    "            scores[\"train_accs\"] = train_accs\n",
    "\n",
    "            if val_set:\n",
    "                scores[\"val_accs\"] = val_accs\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X: np.ndarray, *, threshold=0.5):\n",
    "        \"\"\"\n",
    "         Description:\n",
    "         ------------\n",
    "             Performs prediction after training of the network has been finished.\n",
    "\n",
    "         Parameters:\n",
    "        ------------\n",
    "             I   X (np.ndarray): The design matrix, with n rows of p features each\n",
    "\n",
    "         Optional Parameters:\n",
    "         ------------\n",
    "             II  threshold (float) : sets minimal value for a prediction to be predicted as the positive class\n",
    "                 in classification problems\n",
    "\n",
    "         Returns:\n",
    "         ------------\n",
    "             I   z (np.ndarray): A prediction vector (row) for each row in our design matrix\n",
    "                 This vector is thresholded if regression=False, meaning that classification results\n",
    "                 in a vector of 1s and 0s, while regressions in an array of decimal numbers\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        predict = self._feedforward(X)\n",
    "\n",
    "        if self.classification:\n",
    "            return np.where(predict > threshold, 1, 0)\n",
    "        else:\n",
    "            return predict\n",
    "\n",
    "\n",
    "\n",
    "    def _feedforward(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Calculates the activation of each layer starting at the input and ending at the output.\n",
    "            Each following activation is calculated from a weighted sum of each of the preceeding\n",
    "            activations (except in the case of the input layer).\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   X (np.ndarray): The design matrix, with n rows of p features each\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            I   z (np.ndarray): A prediction vector (row) for each row in our design matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # reset matrices\n",
    "        self.a_matrices = list()\n",
    "        self.z_matrices = list()\n",
    "\n",
    "        # if X is just a vector, make it into a matrix\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1, X.shape[0]))\n",
    "\n",
    "        # Add a coloumn of zeros as the first coloumn of the design matrix, in order\n",
    "        # to add bias to our data\n",
    "        bias = np.ones((X.shape[0], 1)) * 0.01\n",
    "        X = np.hstack([bias, X])\n",
    "\n",
    "        # a^0, the nodes in the input layer (one a^0 for each row in X - where the\n",
    "        # exponent indicates layer number).\n",
    "        a = X\n",
    "        self.a_matrices.append(a)\n",
    "        self.z_matrices.append(a)\n",
    "\n",
    "        # The feed forward algorithm\n",
    "        for i in range(len(self.weights)):\n",
    "            if i < len(self.weights) - 1:\n",
    "                print(a)\n",
    "                z = a @ self.weights[i]\n",
    "                self.z_matrices.append(z)\n",
    "                a = self.hidden_func(z)\n",
    "                # bias column again added to the data here\n",
    "                bias = np.ones((a.shape[0], 1)) * 0.01\n",
    "                a = np.hstack([bias, a])\n",
    "                self.a_matrices.append(a)\n",
    "            else:\n",
    "                try:\n",
    "                    # a^L, the nodes in our output layers\n",
    "                    z = a @ self.weights[i]\n",
    "                    a = self.output_func(z)\n",
    "                    self.a_matrices.append(a)\n",
    "                    self.z_matrices.append(z)\n",
    "                except Exception as OverflowError:\n",
    "                    print(\n",
    "                        \"OverflowError in fit() in FFNN\\nHOW TO DEBUG ERROR: Consider lowering your learning rate or scheduler specific parameters such as momentum, or check if your input values need scaling\"\n",
    "                    )\n",
    "\n",
    "        # this will be a^L\n",
    "        return a\n",
    "\n",
    "\n",
    "    def _accuracy(self, prediction: np.ndarray, target: np.ndarray):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Calculates accuracy of given prediction to target\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   prediction (np.ndarray): vector of predicitons output network\n",
    "                (1s and 0s in case of classification, and real numbers in case of regression)\n",
    "            II  target (np.ndarray): vector of true values (What the network ideally should predict)\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            A floating point number representing the percentage of correctly classified instances.\n",
    "        \"\"\"\n",
    "        assert prediction.size == target.size\n",
    "        return np.average((target == prediction))\n",
    "    def _set_classification(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Decides if FFNN acts as classifier (True) og regressor (False),\n",
    "            sets self.classification during init()\n",
    "        \"\"\"\n",
    "        self.classification = False\n",
    "        if (\n",
    "            self.cost_func.__name__ == \"CostLogReg\"\n",
    "            or self.cost_func.__name__ == \"CostCrossEntropy\"\n",
    "        ):\n",
    "            self.classification = True\n",
    "\n",
    "    def _progress_bar(self, progression, **kwargs):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Displays progress of training\n",
    "        \"\"\"\n",
    "        print_length = 40\n",
    "        num_equals = int(progression * print_length)\n",
    "        num_not = print_length - num_equals\n",
    "        arrow = \">\" if num_equals > 0 else \"\"\n",
    "        bar = \"[\" + \"=\" * (num_equals - 1) + arrow + \"-\" * num_not + \"]\"\n",
    "        perc_print = self._format(progression * 100, decimals=5)\n",
    "        line = f\"  {bar} {perc_print}% \"\n",
    "\n",
    "        for key in kwargs:\n",
    "            if not np.isnan(kwargs[key]):\n",
    "                value = self._format(kwargs[key], decimals=4)\n",
    "                line += f\"| {key}: {value} \"\n",
    "        sys.stdout.write(\"\\r\" + line)\n",
    "        sys.stdout.flush()\n",
    "        return len(line)\n",
    "\n",
    "    def _format(self, value, decimals=4):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Formats decimal numbers for progress bar\n",
    "        \"\"\"\n",
    "        if value > 0:\n",
    "            v = value\n",
    "        elif value < 0:\n",
    "            v = -10 * value\n",
    "        else:\n",
    "            v = 1\n",
    "        n = 1 + math.floor(math.log10(v))\n",
    "        if n >= decimals - 1:\n",
    "            return str(round(value))\n",
    "        return f\"{value:.{decimals-n-1}f}\"\n",
    "\n",
    "\n",
    "#Returns mean squared error\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "#Returns accuracy score, although this one is mostly 0\n",
    "def accuracy_score_numpy(Y_test, Y_pred):\n",
    "    return np.sum(Y_test == Y_pred) / len(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c5c998-7424-49d3-af40-ae89affeebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.46 Learning rate  =  1e-20\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.62  Learning rate  =  1e-20\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.42  Learning rate  =  1e-20\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.51  Learning rate  =  1e-20\n",
      "Lambda =  400.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [==============>-------------------------] 39.72% | train_error: 2.52   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [=======================================>] 100.0% | train_error: 2.52 Learning rate  =  1e-20\n",
      "Lambda =  5000.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.61 Learning rate  =  1e-16\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.70  Learning rate  =  1e-16\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.64  Learning rate  =  1e-16\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.46  Learning rate  =  1e-16\n",
      "Lambda =  400.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======>--------------------------------] 21.44% | train_error: 2.52   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [=======================================>] 100.0% | train_error: 2.52 Learning rate  =  1e-16\n",
      "Lambda =  5000.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.61 Learning rate  =  1e-14\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.62  Learning rate  =  1e-14\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.56  Learning rate  =  1e-14\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.57  Learning rate  =  1e-14\n",
      "Lambda =  400.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [==>-------------------------------------] 9.120% | train_error: 2.59   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [=======================================>] 100.0% | train_error: 2.59 Learning rate  =  1e-14\n",
      "Lambda =  5000.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.63 Learning rate  =  1e-10\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.50  Learning rate  =  1e-10\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [==============================>---------] 78.33% | train_error: 2.62  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [====================================>---] 94.56% | train_error: 2.42 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [==================================>-----] 88.46% | train_error: 2.43 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "#initialize parameters. Might need some normalization.\n",
    "\n",
    "poly_deg = 3 #polynomial degree\n",
    "xlim = 1.0 #range of x values goes from 0 to xlim\n",
    "np.random.seed(42) #creates random seed to have reproducible results\n",
    "\n",
    "n = 1000 # number of data points\n",
    "step = 1/n # step size in x\n",
    "x = np.arange(0, xlim, step) # create data points\n",
    "target = FrankeFunction(x) # create y-data\n",
    "target = target.reshape(target.shape[0], 1) # reshaping y-data\n",
    "y = target # creating a copy of the target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = createX(x,poly_deg) # creating design matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y) # splitting data\n",
    "\n",
    "\n",
    "n_output_nodes = 1 #s et to 1 for regression\n",
    "n_input_nodes = X_train.shape[1] # set number of input nodes\n",
    "n_hidden_nodes = 5 # set number of hidden nodes\n",
    "\n",
    "FFNN1 = FFNN(dimensions = [n_input_nodes,n_hidden_nodes,n_output_nodes],hidden_func = sigmoid) #creates an object of the neural network\n",
    "\n",
    "\n",
    "scheduler = Adagrad(eta=1e-3) # creates a schedueler which controls the learning rate decay. Set this equal to the scheduele mode, i.e. Adagrad, Constant, ADAM, with the paramaters requiered for this scheduele\n",
    "\n",
    "varlist = np.logspace(-9,-.0005, 1) #list of variables to be tested. Set third float to 1 if nothing is to be tested\n",
    "MSElist = [] #list of MSE. Not sure if this is usefull.\n",
    "\n",
    "M = 10\n",
    "lmbd_vals = [1E2,2E2,3E2,4E2,5E3] \n",
    "eta_vals = [1e-20, 1e-16, 1e-14, 1e-10, 1e-1] #Can be adjusted, but code gets very slow for eta below 1e-10\n",
    "epochs = 100\n",
    "\n",
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "\n",
    "for i, etaa in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals): # starts a loop to test variables.\n",
    "        FFNN1.fit(X_train,y_train, Adagrad(eta = (etaa)), epochs = int(lmbd)) #train data to the fit. Using the training data here, not sure if this is the best option. Many parameters can be set to optimize the fit, such as eta, lambda, momentum.\n",
    "        #DNN_numpy[i][j] = fitted\n",
    "        train_pred = FFNN1.predict(X_train) #creates a prediction from the training. Hopefully this is working, but not completely sure.\n",
    "        test_pred = FFNN1.predict(X_test)\n",
    "        #score = FFNN1._accuracy(y,y2) #supposed to give accuracy score of the fit, returns 0\n",
    "\n",
    "        MSE_train = mean_squared_error(y_train, train_pred) #return MSE of the fit.\n",
    "        MSE_test = mean_squared_error(y_test, test_pred)\n",
    "        \n",
    "        train_accuracy[i][j] = MSE_train\n",
    "        test_accuracy[i][j] = MSE_test\n",
    "        print(\"Learning rate  = \", etaa)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", mean_squared_error(y_test, test_pred))\n",
    "\n",
    "\n",
    "# Grid search and visualization own code\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "M = 10\n",
    "lmbd_vals = [1e-20, 1e-15, 1e-10, 1e-5, 1e0] \n",
    "eta_vals = [1e-20, 1e-16, 1e-14, 1e-10, 1e-6] #Can be adjusted, but code gets very slow for eta below 1e-10\n",
    "epochs = 1000000\n",
    "\n",
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "# grid search\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        FFNN1.reset_weights() #important to reset weights, as to not generate data based on previous runs\n",
    "        scores = FFNN1.fit(X_train,y_train, Adagrad(eta = eta), epochs = 1000) #train data to the fit. Using the training data here, not sure if this is the best option. Many parameters can be set to optimize the fit, such as eta, lambda, momentum.   \n",
    "        DNN_numpy[i][j] = FFNN1\n",
    "        test_predict = FFNN1.predict(X) #creates a prediction from the training.\n",
    "        score = FFNN1._accuracy(y,test_predict)\n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", mean_squared_error(y, test_predict))\n",
    "\n",
    "train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "\n",
    "for i in range(len(eta_vals)):\n",
    "    for j in range(len(lmbd_vals)):\n",
    "        dnn = DNN_numpy[i][j]\n",
    "        train_pred = dnn.predict(X_train) \n",
    "        test_pred = dnn.predict(X_test)\n",
    "        train_accuracy[i][j] = mean_squared_error(y_train, train_pred)\n",
    "        test_accuracy[i][j] = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=\"viridis_r\", xticklabels=lmbd_vals, yticklabels=eta_vals)\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"eta\")\n",
    "ax.set_xlabel(\"lambda\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis_r\", xticklabels=lmbd_vals, yticklabels=eta_vals)\n",
    "ax.set_title(\"Test Accuracy\")\n",
    "ax.set_ylabel(\"eta\")\n",
    "ax.set_xlabel(\"lambda\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857e2b1-ad91-4a0d-8507-a22bda0d8c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
