{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e351149c-39fd-4571-a52b-57164fcbab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4dd48-bcd8-41fd-924e-05f6d228e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "n = 100\n",
    "x = 2*np.random.rand(n,1)\n",
    "y = 4+3*x+np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x]\n",
    "beta_linreg = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "print(beta_linreg)\n",
    "sgdreg = SGDRegressor(max_iter = 50, penalty=None, eta0=0.1)\n",
    "sgdreg.fit(x,y.ravel())\n",
    "print(sgdreg.intercept_, sgdreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299359eb-1a0a-4a06-8d24-dec2c4c9b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of the breast cancer dataset is:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "-------------------------\n",
      "inputs =  (569, 30)\n",
      "outputs =  (569,)\n",
      "labels =  (30,)\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset and creating Design Matrix\n",
    "np.random.seed(0)        #create same seed for random number every time\n",
    "cancer=load_breast_cancer()      #Download breast cancer dataset\n",
    "inputs=cancer.data                     #Feature matrix of 569 rows (samples) and 30 columns (parameters)\n",
    "outputs=cancer.target                  #Label array of 569 rows (0 for benign and 1 for malignant)\n",
    "labels=cancer.feature_names[0:30]\n",
    "\n",
    "#Print information about the datasets\n",
    "print('The content of the breast cancer dataset is:')\n",
    "print(labels)\n",
    "print('-------------------------')\n",
    "print(\"inputs =  \" + str(inputs.shape))\n",
    "print(\"outputs =  \" + str(outputs.shape))\n",
    "print(\"labels =  \"+ str(labels.shape))\n",
    "\n",
    "def standard_scale(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std_dev = np.std(data, axis=0)\n",
    "    scaled_data = (data - mean) / std_dev\n",
    "    return scaled_data, mean, std_dev\n",
    "\n",
    "x = inputs \n",
    "x, mean, std_dev = standard_scale(x)\n",
    "y = outputs\n",
    "temp1 = np.reshape(x[:,1], (len(x[:,1]),1))\n",
    "temp2 = np.reshape(x[:,2], (len(x[:,2]),1))\n",
    "X = np.hstack((temp1, temp2))      \n",
    "temp = np.reshape(x[:,5], (len(x[:,5]),1))\n",
    "X = np.hstack((X,temp))       \n",
    "temp = np.reshape(x[:,8], (len(x[:,8]),1))\n",
    "X = np.hstack((X,temp))\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "#X_train, X_test, y_train, y_test = train_test_split_numpy(X, y, 0.7, 0.3)   #Split datasets into training and testing\n",
    "del temp1,temp2,temp\n",
    "#y_train_onehot = to_categorical_numpy(y_train)\n",
    "#y_test_onehot = to_categorical_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeccdca1-c72d-48d5-b366-4298f17be547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Cost function\n",
    "def CostCrossEntropy(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return -(1.0 / target.size) * np.sum(target * np.log(X + 10e-10))\n",
    "\n",
    "    return func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527d313e-55f5-4031-987e-84066e20c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns mean squared error\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "#Returns accuracy score, although this one is mostly 0\n",
    "def accuracy_score_numpy(Y_test, Y_pred):\n",
    "    return np.sum(Y_test == Y_pred) / len(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89e513c9-5490-4ed3-8c68-1778fd0f1319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own inversion\n",
      "[[-0.08685965]\n",
      " [-0.27459236]\n",
      " [-0.08268545]\n",
      " [-0.0535042 ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CostCrossEnthropy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(theta_linreg)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Note that we request the derivative wrt third argument (theta, 2 here)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m training_gradient \u001b[38;5;241m=\u001b[39m grad(\u001b[43mCostCrossEnthropy\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define parameters for Stochastic Gradient Descent\u001b[39;00m\n\u001b[1;32m     13\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m1000\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CostCrossEnthropy' is not defined"
     ]
    }
   ],
   "source": [
    "# Using Autograd to calculate gradients using ADAM and Stochastic Gradient descent\n",
    "\n",
    "#X = np.c_[np.ones((n,1)), x, x*x]\n",
    "XT_X = X.T @ X\n",
    "theta_linreg = np.linalg.pinv(XT_X) @ (X.T @ y)\n",
    "print(\"Own inversion\")\n",
    "print(theta_linreg)\n",
    "\n",
    "\n",
    "# Note that we request the derivative wrt third argument (theta, 2 here)\n",
    "training_gradient = grad(CostCrossEnthropy)\n",
    "# Define parameters for Stochastic Gradient Descent\n",
    "n_epochs = [1,10,100,1000]\n",
    "M = 10   #size of each minibatch\n",
    "m = int(n/M) #number of minibatches\n",
    "# Guess for unknown parameters theta\n",
    "theta = np.random.randn(3,1)\n",
    "lmbd = 0.01\n",
    "\n",
    "# Value for learning rate\n",
    "eta_vals = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "# Value for parameters beta1 and beta2, see https://arxiv.org/abs/1412.6980\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "# Including AdaGrad parameter to avoid possible division by zero\n",
    "delta  = 1e-7\n",
    "iter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    first_moment = 0.0\n",
    "    second_moment = 0.0\n",
    "    iter += 1\n",
    "    for i in range(m):\n",
    "        random_index = M*np.random.randint(m)\n",
    "        xi = X[random_index:random_index+M]\n",
    "        yi = y[random_index:random_index+M]\n",
    "        gradients = (1.0/M)*training_gradient(yi, xi, theta)\n",
    "        # Computing moments first\n",
    "        first_moment = beta1*first_moment + (1-beta1)*gradients\n",
    "        second_moment = beta2*second_moment+(1-beta2)*gradients*gradients\n",
    "        first_term = first_moment/(1.0-beta1**iter)\n",
    "        second_term = second_moment/(1.0-beta2**iter)\n",
    "\t# Scaling with rho the new and the previous results\n",
    "        update = eta*first_term/(np.sqrt(second_term)+delta)\n",
    "        theta -= update\n",
    "print(\"theta from own ADAM\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377b100-3f83-4d6a-843e-f6da16be2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 12   \n",
    "\n",
    "train_accuracy *= 100\n",
    "test_accuracy *= 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=\"viridis\", xticklabels=epochs, yticklabels=eta_vals, fmt=\".1f\",  annot_kws={\"size\": fontsize},  vmin=0, vmax=100)\n",
    "ax.set_title(\"Training Accuracy Own Code [%]\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=fontsize)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "plt.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.savefig('/home/Kristen/Documents/FYS4155/Excecises/C_own_EpoLR_Train.pdf', format='pdf', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "sns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis\", xticklabels=epochs, yticklabels=eta_vals, fmt=\".1f\",  annot_kws={\"size\": fontsize},  vmin=0, vmax=100)\n",
    "ax.set_title(\"Test Accuracy Own Code [%]\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Learning Rate\", fontsize=fontsize)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=fontsize)\n",
    "plt.tick_params(axis='both', labelsize=fontsize)\n",
    "plt.savefig('/home/Kristen/Documents/FYS4155/Excecises/C_own_EpoLR_Test.pdf', format='pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815957c1-a1d2-4dda-bf56-1745944c9dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic gradient descent\n",
    "n_epochs = int(n/5)\n",
    "M = int(100)\n",
    "eta = 0.1\n",
    "#SGD function takes x,y, number of epochs, size of minibatch, number datapoints, and the name of the learning rate modifier\n",
    "beta, beta_linreg = SGD(x,y, n_epochs, M, n, \"ADAGRA\", eta)\n",
    "xbnew = np.c_[np.ones((len(x),1)), x, x**2, x**3, x**4, x**5, x**6]\n",
    "ypred = xbnew @ beta\n",
    "ypred2 = xbnew @ beta_linreg\n",
    "plt.plot(x,ypred, 'ro')\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c5c998-7424-49d3-af40-ae89affeebd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.46 Learning rate  =  1e-20\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.62  Learning rate  =  1e-20\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.42  Learning rate  =  1e-20\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.51  Learning rate  =  1e-20\n",
      "Lambda =  400.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-20, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.52   Learning rate  =  1e-20\n",
      "Lambda =  5000.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.61 Learning rate  =  1e-16\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.70  Learning rate  =  1e-16\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.64  Learning rate  =  1e-16\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.46  Learning rate  =  1e-16\n",
      "Lambda =  400.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-16, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.52   Learning rate  =  1e-16\n",
      "Lambda =  5000.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.61 Learning rate  =  1e-14\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.62  Learning rate  =  1e-14\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.56  Learning rate  =  1e-14\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.57  Learning rate  =  1e-14\n",
      "Lambda =  400.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-14, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.59   Learning rate  =  1e-14\n",
      "Lambda =  5000.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.63 Learning rate  =  1e-10\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.50  Learning rate  =  1e-10\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.62  Learning rate  =  1e-10\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.54  Learning rate  =  1e-10\n",
      "Lambda =  400.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=1e-10, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.42   Learning rate  =  1e-10\n",
      "Lambda =  5000.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=0.1, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.51 Learning rate  =  0.1\n",
      "Lambda =  100.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=0.1, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.53  Learning rate  =  0.1\n",
      "Lambda =  200.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=0.1, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.50  Learning rate  =  0.1\n",
      "Lambda =  300.0\n",
      "Accuracy score on test set:  0.5356897750042423\n",
      "Adagrad: Eta=0.1, Lambda=0\n",
      "  [=============>--------------------------] 36.75% | train_error: 2.57  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [=====================================>--] 96.60% | train_error: 2.43 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "#initialize parameters. Might need some normalization.\n",
    "\n",
    "poly_deg = 3 #polynomial degree\n",
    "xlim = 1.0 #range of x values goes from 0 to xlim\n",
    "np.random.seed(42) #creates random seed to have reproducible results\n",
    "\n",
    "n = 1000 # number of data points\n",
    "step = 1/n # step size in x\n",
    "x = np.arange(0, xlim, step) # create data points\n",
    "target = FrankeFunction(x) # create y-data\n",
    "target = target.reshape(target.shape[0], 1) # reshaping y-data\n",
    "y = target # creating a copy of the target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = createX(x,poly_deg) # creating design matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y) # splitting data\n",
    "\n",
    "\n",
    "n_output_nodes = 1 #s et to 1 for regression\n",
    "n_input_nodes = X_train.shape[1] # set number of input nodes\n",
    "n_hidden_nodes = 5 # set number of hidden nodes\n",
    "\n",
    "FFNN1 = FFNN(dimensions = [n_input_nodes,n_hidden_nodes,n_output_nodes],hidden_func = sigmoid) #creates an object of the neural network\n",
    "\n",
    "\n",
    "scheduler = Adagrad(eta=1e-3) # creates a schedueler which controls the learning rate decay. Set this equal to the scheduele mode, i.e. Adagrad, Constant, ADAM, with the paramaters requiered for this scheduele\n",
    "\n",
    "varlist = np.logspace(-9,-.0005, 1) #list of variables to be tested. Set third float to 1 if nothing is to be tested\n",
    "MSElist = [] #list of MSE. Not sure if this is usefull.\n",
    "\n",
    "M = 10\n",
    "lmbd_vals = [1E2,2E2,3E2,4E2,5E3] \n",
    "eta_vals = [1e-20, 1e-16, 1e-14, 1e-10, 1e-1] #Can be adjusted, but code gets very slow for eta below 1e-10\n",
    "epochs = 100\n",
    "\n",
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "\n",
    "for i, etaa in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals): # starts a loop to test variables.\n",
    "        FFNN1.fit(X_train,y_train, Adagrad(eta = (etaa)), epochs = int(lmbd)) #train data to the fit. Using the training data here, not sure if this is the best option. Many parameters can be set to optimize the fit, such as eta, lambda, momentum.\n",
    "        #DNN_numpy[i][j] = fitted\n",
    "        train_pred = FFNN1.predict(X_train) #creates a prediction from the training. Hopefully this is working, but not completely sure.\n",
    "        test_pred = FFNN1.predict(X_test)\n",
    "        #score = FFNN1._accuracy(y,y2) #supposed to give accuracy score of the fit, returns 0\n",
    "\n",
    "        MSE_train = mean_squared_error(y_train, train_pred) #return MSE of the fit.\n",
    "        MSE_test = mean_squared_error(y_test, test_pred)\n",
    "        \n",
    "        train_accuracy[i][j] = MSE_train\n",
    "        test_accuracy[i][j] = MSE_test\n",
    "        print(\"Learning rate  = \", etaa)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", mean_squared_error(y_test, test_pred))\n",
    "\n",
    "\n",
    "# Grid search and visualization own code\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "M = 10\n",
    "lmbd_vals = [1e-20, 1e-15, 1e-10, 1e-5, 1e0] \n",
    "eta_vals = [1e-20, 1e-16, 1e-14, 1e-10, 1e-6] #Can be adjusted, but code gets very slow for eta below 1e-10\n",
    "epochs = 1000000\n",
    "\n",
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "# grid search\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        FFNN1.reset_weights() #important to reset weights, as to not generate data based on previous runs\n",
    "        scores = FFNN1.fit(X_train,y_train, Adagrad(eta = eta), epochs = 1000) #train data to the fit. Using the training data here, not sure if this is the best option. Many parameters can be set to optimize the fit, such as eta, lambda, momentum.   \n",
    "        DNN_numpy[i][j] = FFNN1\n",
    "        test_predict = FFNN1.predict(X) #creates a prediction from the training.\n",
    "        score = FFNN1._accuracy(y,test_predict)\n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", mean_squared_error(y, test_predict))\n",
    "\n",
    "train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "test_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\n",
    "\n",
    "for i in range(len(eta_vals)):\n",
    "    for j in range(len(lmbd_vals)):\n",
    "        dnn = DNN_numpy[i][j]\n",
    "        train_pred = dnn.predict(X_train) \n",
    "        test_pred = dnn.predict(X_test)\n",
    "        train_accuracy[i][j] = mean_squared_error(y_train, train_pred)\n",
    "        test_accuracy[i][j] = mean_squared_error(y_test, test_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=\"viridis_r\", xticklabels=lmbd_vals, yticklabels=eta_vals)\n",
    "ax.set_title(\"Training Accuracy\")\n",
    "ax.set_ylabel(\"eta\")\n",
    "ax.set_xlabel(\"lambda\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "sns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis_r\", xticklabels=lmbd_vals, yticklabels=eta_vals)\n",
    "ax.set_title(\"Test Accuracy\")\n",
    "ax.set_ylabel(\"eta\")\n",
    "ax.set_xlabel(\"lambda\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274f603-c3a6-40d5-a47c-55a14ff231ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    try:\n",
    "        return 1.0 / (1 + np.exp(-X))\n",
    "    except FloatingPointError:\n",
    "        return np.where(X > np.zeros(X.shape), np.ones(X.shape), np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    X = X - np.max(X, axis=-1, keepdims=True)\n",
    "    delta = 10e-10\n",
    "    return np.exp(X) / (np.sum(np.exp(X), axis=-1, keepdims=True) + delta)\n",
    "\n",
    "\n",
    "def RELU(X):\n",
    "    return np.where(X > np.zeros(X.shape), X, np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def LRELU(X):\n",
    "    delta = 10e-4\n",
    "    return np.where(X > np.zeros(X.shape), X, delta * X)\n",
    "\n",
    "\n",
    "def SGD(x,y,n_epochs,M,n, learningMod, eta):\n",
    "    X = np.c_[np.ones((n,1)),x,x**2,x**3, x**4, x**5, x**6]\n",
    "    m = int(n/M)\n",
    "    t0 = M\n",
    "    t1 = n_epochs\n",
    "    beta_linreg = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    beta = np.random.randn(len(X[1]),1)\n",
    "    training_gradient = grad(cost)\n",
    "    change = 0.0\n",
    "    delta = 1E-8\n",
    "    momentum = 0.2\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    rho = 0.99\n",
    "    iter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        Giter = 0.0\n",
    "        first_moment = 0.0\n",
    "        second_moment = 0.0\n",
    "        iter += 1\n",
    "        for i in range(m):\n",
    "            random_index = M*np.random.randint(m)\n",
    "            xi = X[random_index:random_index+M]\n",
    "            yi = y[random_index:random_index+M]\n",
    "            \n",
    "            gradients = gradient(xi,yi,M,beta)\n",
    "            #gradients = training_gradient(xi,yi,beta)\n",
    "            \n",
    "            if(learningMod == \"ADAGRAD\"):\n",
    "                Giter += gradients**2\n",
    "                new_change = (eta*gradients.reshape(beta.shape))/(delta+np.sqrt(Giter)) + momentum * change\n",
    "            if(learningMod == \"RMSprop\"):\n",
    "                Giter = (rho*Giter+(1-rho)*gradients*gradients)\n",
    "                new_change = (eta*gradients.reshape(beta.shape))/(delta+np.sqrt(Giter)) + momentum * change\n",
    "            if(learningMod == \"ADAM\"):\n",
    "                first_moment = beta1*first_moment + (1-beta1)*gradients\n",
    "                second_moment = beta2*second_moment+(1-beta2)*gradients*gradients\n",
    "                first_term = first_moment/(1.0-beta1**iter)\n",
    "                second_term = second_moment/(1.0-beta2**iter)\n",
    "                new_change = eta*first_term/(np.sqrt(second_term)+delta) + momentum * change\n",
    "            else:\n",
    "                new_change = eta*gradients.reshape(beta.shape) + momentum * change\n",
    "            beta -= new_change\n",
    "            change = new_change\n",
    "            \n",
    "            \n",
    "    return(beta, beta_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857e2b1-ad91-4a0d-8507-a22bda0d8c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
